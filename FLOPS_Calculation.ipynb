{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgLoaWuPGEEA8x/UN/Prvg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Djinho/EvoNet-CNN-Insight/blob/main/FLOPS_Calculation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, regularizers\n",
        "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2"
      ],
      "metadata": {
        "id": "5FNnV2UYIxb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline"
      ],
      "metadata": {
        "id": "Opw6ww3sIzvY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LblBrwQvg0Qd",
        "outputId": "fd0aef73-135d-4803-9d3a-b40c788ed8bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs: 272125313\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Define the model with the updated input shape\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.005, l2=0.005), padding='valid', input_shape=(198, 192, 1)),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.005, l2=0.005), padding='valid'),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.005, l2=0.005), padding='valid'),\n",
        "    layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(units=128, activation='relu'),\n",
        "    layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Convert model to concrete function\n",
        "model_func = tf.function(lambda x: model(x))\n",
        "model_func = model_func.get_concrete_function(tf.TensorSpec([1] + list(model.input_shape[1:])))\n",
        "\n",
        "# Get frozen concrete function\n",
        "frozen_func = convert_variables_to_constants_v2(model_func)\n",
        "frozen_func.graph.as_graph_def()\n",
        "\n",
        "# Calculate FLOPs and MACs\n",
        "run_meta = tf.compat.v1.RunMetadata()\n",
        "opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
        "\n",
        "# Use the profiler to calculate FLOPs\n",
        "flops = tf.compat.v1.profiler.profile(graph=frozen_func.graph,\n",
        "                                      run_meta=run_meta, cmd='op', options=opts)\n",
        "\n",
        "print(f\"FLOPs: {flops.total_float_ops}\")\n",
        "#2.721 × 10⁸.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recent and weak flops count"
      ],
      "metadata": {
        "id": "FJMGauh3I5nG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build_best_model(input_shape):\n",
        "    # Best hyperparameters from Optuna optimization\n",
        "    num_layers = 4\n",
        "    filters = [39, 53, 94, 101]\n",
        "    kernel_size = 2\n",
        "    l1 = 7.734333542494201e-05\n",
        "    l2 = 6.536870301639723e-05\n",
        "    dense_units = 238\n",
        "    dropout_rate = 0.29358511911026797\n",
        "    learning_rate = 5.723121393552307e-05\n",
        "\n",
        "    # Building the model based on the best hyperparameters\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Input(shape=input_shape))  # Define the input layer\n",
        "    for i in range(num_layers):\n",
        "        model.add(layers.Conv2D(filters=filters[i], kernel_size=(kernel_size, kernel_size), strides=(1, 1), activation='relu',\n",
        "                                kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), padding='same'))  # Use 'same' padding to avoid dimension issues\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())  # Flatten the output for the Dense layers\n",
        "    model.add(layers.Dense(units=dense_units, activation='relu'))  # Add Dense layer\n",
        "    model.add(layers.Dropout(rate=dropout_rate))  # Add Dropout layer\n",
        "    model.add(layers.Dense(units=1, activation='sigmoid'))  # Output layer with sigmoid activation\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)  # Define optimizer\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])  # Compile the model\n",
        "\n",
        "    return model\n",
        "\n",
        "# Assuming gene_sim.data is already preprocessed\n",
        "input_shape = (198, 192, 1)  # Example input shape\n",
        "\n",
        "# Build the model\n",
        "model = build_best_model(input_shape)\n",
        "\n",
        "# Convert model to concrete function\n",
        "model_func = tf.function(lambda x: model(x))\n",
        "model_func = model_func.get_concrete_function(tf.TensorSpec([1] + list(model.input_shape[1:])))\n",
        "\n",
        "# Get frozen concrete function\n",
        "frozen_func = convert_variables_to_constants_v2(model_func)\n",
        "frozen_func.graph.as_graph_def()\n",
        "\n",
        "# Calculate FLOPs and MACs\n",
        "run_meta = tf.compat.v1.RunMetadata()\n",
        "opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
        "\n",
        "# Use the profiler to calculate FLOPs\n",
        "flops = tf.compat.v1.profiler.profile(graph=frozen_func.graph,\n",
        "                                      run_meta=run_meta, cmd='op', options=opts)\n",
        "\n",
        "print(f\"FLOPs: {flops.total_float_ops}\")\n",
        "#3.17954059×10 to he power 8\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnY2WSOvI8Fs",
        "outputId": "603ccb7f-5e57-49d6-d65a-bb5ffeab2736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/nn_ops.py:5256: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs: 317954059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "recent and moderate flops count"
      ],
      "metadata": {
        "id": "6OVRYqv3htAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build_best_model(input_shape):\n",
        "    # Best hyperparameters from Optuna optimization\n",
        "    num_layers = 5\n",
        "    filters = [85, 119, 99, 115, 54]\n",
        "    kernel_size = 3\n",
        "    l1 = 0.00020959985382232806\n",
        "    l2 = 0.00010741242396156945\n",
        "    dense_units = 69\n",
        "    dropout_rate = 0.29601406545155096\n",
        "    learning_rate = 0.00013314057394016722\n",
        "\n",
        "    # Building the model based on the best hyperparameters\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Input(shape=input_shape))  # Define the input layer\n",
        "    for i in range(num_layers):\n",
        "        model.add(layers.Conv2D(filters=filters[i], kernel_size=(kernel_size, kernel_size), strides=(1, 1), activation='relu',\n",
        "                                kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), padding='same'))  # Use 'same' padding to avoid dimension issues\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())  # Flatten the output for the Dense layers\n",
        "    model.add(layers.Dense(units=dense_units, activation='relu'))  # Add Dense layer\n",
        "    model.add(layers.Dropout(rate=dropout_rate))  # Add Dropout layer\n",
        "    model.add(layers.Dense(units=1, activation='sigmoid'))  # Output layer with sigmoid activation\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)  # Define optimizer\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])  # Compile the model\n",
        "\n",
        "    return model\n",
        "\n",
        "# Assuming gene_sim.data is already preprocessed\n",
        "input_shape = (198, 192, 1)  # Example input shape\n",
        "\n",
        "# Build the model\n",
        "model = build_best_model(input_shape)\n",
        "\n",
        "# Convert model to concrete function\n",
        "model_func = tf.function(lambda x: model(x))\n",
        "model_func = model_func.get_concrete_function(tf.TensorSpec([1] + list(model.input_shape[1:])))\n",
        "\n",
        "# Get frozen concrete function\n",
        "frozen_func = convert_variables_to_constants_v2(model_func)\n",
        "frozen_func.graph.as_graph_def()\n",
        "\n",
        "# Calculate FLOPs and MACs\n",
        "run_meta = tf.compat.v1.RunMetadata()\n",
        "opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
        "\n",
        "# Use the profiler to calculate FLOPs\n",
        "flops = tf.compat.v1.profiler.profile(graph=frozen_func.graph,\n",
        "                                      run_meta=run_meta, cmd='op', options=opts)\n",
        "\n",
        "print(f\"FLOPs: {flops.total_float_ops}\")\n",
        "# 2.431 × 10⁹."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihGUhnnJh2Yw",
        "outputId": "f9e75aa7-8271-49ed-c9b0-4486ccc9b8ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs: 2431044880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intermeidate and weak flops"
      ],
      "metadata": {
        "id": "At1lz93oq8od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, regularizers\n",
        "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
        "\n",
        "def build_best_model(input_shape):\n",
        "    # Best hyperparameters from Optuna optimization\n",
        "    num_layers = 6\n",
        "    filters = [36, 61, 125, 58, 46, 80]\n",
        "    kernel_size = 2\n",
        "    l1 = 5.7944957902052485e-05\n",
        "    l2 = 0.00013078230073021723\n",
        "    dense_units = 152\n",
        "    dropout_rate = 0.24228689196387476\n",
        "    learning_rate = 0.0003981287110754072\n",
        "\n",
        "    # Building the model based on the best hyperparameters\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Input(shape=input_shape))  # Define the input layer\n",
        "    for i in range(num_layers):\n",
        "        model.add(layers.Conv2D(filters=filters[i], kernel_size=(kernel_size, kernel_size), strides=(1, 1), activation='relu',\n",
        "                                kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), padding='same'))  # Use 'same' padding to avoid dimension issues\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())  # Flatten the output for the Dense layers\n",
        "    model.add(layers.Dense(units=dense_units, activation='relu'))  # Add Dense layer\n",
        "    model.add(layers.Dropout(rate=dropout_rate))  # Add Dropout layer\n",
        "    model.add(layers.Dense(units=1, activation='sigmoid'))  # Output layer with sigmoid activation\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)  # Define optimizer\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])  # Compile the model\n",
        "\n",
        "    return model\n",
        "\n",
        "# Assuming gene_sim.data is already preprocessed and input_shape is known\n",
        "input_shape = (198, 192, 1)  # Example input shape\n",
        "\n",
        "# Build the model\n",
        "model = build_best_model(input_shape)\n",
        "\n",
        "# Convert model to concrete function\n",
        "model_func = tf.function(lambda x: model(x))\n",
        "model_func = model_func.get_concrete_function(tf.TensorSpec([1] + list(model.input_shape[1:])))\n",
        "\n",
        "# Get frozen concrete function\n",
        "frozen_func = convert_variables_to_constants_v2(model_func)\n",
        "frozen_func.graph.as_graph_def()\n",
        "\n",
        "# Calculate FLOPs and MACs\n",
        "run_meta = tf.compat.v1.RunMetadata()\n",
        "opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
        "\n",
        "# Use the profiler to calculate FLOPs\n",
        "flops = tf.compat.v1.profiler.profile(graph=frozen_func.graph,\n",
        "                                      run_meta=run_meta, cmd='op', options=opts)\n",
        "\n",
        "print(f\"FLOPs: {flops.total_float_ops}\")\n",
        "# 3.637 × 10⁸.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R329gdaq_DH",
        "outputId": "257b34be-8256-47a4-aa19-992678cc8a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs: 363706201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intermeidate and moderate flops"
      ],
      "metadata": {
        "id": "EhKoZBk8tDmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build_best_model(input_shape):\n",
        "    # Best hyperparameters from Optuna optimization\n",
        "    num_layers = 3\n",
        "    filters = [79, 83, 81]\n",
        "    kernel_size = 2\n",
        "    l1 = 2.4768594237307907e-05\n",
        "    l2 = 0.0006992137199417164\n",
        "    dense_units = 108\n",
        "    dropout_rate = 0.18570184832201814\n",
        "    learning_rate = 0.00018161195979508115\n",
        "\n",
        "    # Building the model based on the best hyperparameters\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Input(shape=input_shape))  # Define the input layer\n",
        "    for i in range(num_layers):\n",
        "        model.add(layers.Conv2D(filters=filters[i], kernel_size=(kernel_size, kernel_size), strides=(1, 1), activation='relu',\n",
        "                                kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), padding='same'))  # Use 'same' padding to avoid dimension issues\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())  # Flatten the output for the Dense layers\n",
        "    model.add(layers.Dense(units=dense_units, activation='relu'))  # Add Dense layer\n",
        "    model.add(layers.Dropout(rate=dropout_rate))  # Add Dropout layer\n",
        "    model.add(layers.Dense(units=1, activation='sigmoid'))  # Output layer with sigmoid activation\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)  # Define optimizer\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])  # Compile the model\n",
        "\n",
        "    return model\n",
        "\n",
        "# Assuming gene_sim.data is already preprocessed and input_shape is known\n",
        "input_shape = (198, 192, 1)  # Replace with your actual input shape\n",
        "\n",
        "# Build the model\n",
        "model = build_best_model(input_shape)\n",
        "\n",
        "# Convert model to concrete function\n",
        "model_func = tf.function(lambda x: model(x))\n",
        "model_func = model_func.get_concrete_function(tf.TensorSpec([1] + list(model.input_shape[1:])))\n",
        "\n",
        "# Get frozen concrete function\n",
        "frozen_func = convert_variables_to_constants_v2(model_func)\n",
        "frozen_func.graph.as_graph_def()\n",
        "\n",
        "# Calculate FLOPs and MACs\n",
        "run_meta = tf.compat.v1.RunMetadata()\n",
        "opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
        "\n",
        "# Use the profiler to calculate FLOPs\n",
        "flops = tf.compat.v1.profiler.profile(graph=frozen_func.graph,\n",
        "                                      run_meta=run_meta, cmd='op', options=opts)\n",
        "\n",
        "print(f\"FLOPs: {flops.total_float_ops}\")\n",
        "#6.671 × 10⁸.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7-wKjA5tIoD",
        "outputId": "6d183a8f-47d8-429a-cdb2-d463eed69e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs: 667099285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intermediate strong"
      ],
      "metadata": {
        "id": "qkW3OiNbxXhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build_best_model(input_shape):\n",
        "    # Best hyperparameters from Optuna optimization\n",
        "    num_layers = 4\n",
        "    filters = [124, 107, 37, 107]\n",
        "    kernel_size = 3\n",
        "    l1 = 0.00023342282213110015\n",
        "    l2 = 6.698426243226582e-06\n",
        "    dense_units = 205\n",
        "    dropout_rate = 0.3223287518244954\n",
        "    learning_rate = 0.00011551960587189376\n",
        "\n",
        "    # Building the model based on the best hyperparameters\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Input(shape=input_shape))  # Define the input layer\n",
        "    for i in range(num_layers):\n",
        "        model.add(layers.Conv2D(filters=filters[i], kernel_size=(kernel_size, kernel_size), strides=(1, 1), activation='relu',\n",
        "                                kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), padding='same'))  # Use 'same' padding to avoid dimension issues\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())  # Flatten the output for the Dense layers\n",
        "    model.add(layers.Dense(units=dense_units, activation='relu'))  # Add Dense layer\n",
        "    model.add(layers.Dropout(rate=dropout_rate))  # Add Dropout layer\n",
        "    model.add(layers.Dense(units=1, activation='sigmoid'))  # Output layer with sigmoid activation\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)  # Define optimizer\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])  # Compile the model\n",
        "\n",
        "    return model\n",
        "\n",
        "# Assuming gene_sim.data is already preprocessed and input_shape is known\n",
        "input_shape = (198, 192, 1)  # Example input shape, update this if necessary\n",
        "\n",
        "# Build the model\n",
        "model = build_best_model(input_shape)\n",
        "\n",
        "# Convert model to concrete function\n",
        "model_func = tf.function(lambda x: model(x))\n",
        "model_func = model_func.get_concrete_function(tf.TensorSpec([1] + list(model.input_shape[1:])))\n",
        "\n",
        "# Get frozen concrete function\n",
        "frozen_func = convert_variables_to_constants_v2(model_func)\n",
        "frozen_func.graph.as_graph_def()\n",
        "\n",
        "# Calculate FLOPs\n",
        "run_meta = tf.compat.v1.RunMetadata()\n",
        "opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
        "\n",
        "# Use the profiler to calculate FLOPs\n",
        "flops = tf.compat.v1.profiler.profile(graph=frozen_func.graph,\n",
        "                                      run_meta=run_meta, cmd='op', options=opts)\n",
        "\n",
        "print(f\"FLOPs: {flops.total_float_ops}\")\n",
        "#2.581 × 10⁹.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2wLOcCvxZHf",
        "outputId": "292f591c-d2f9-4dfa-8808-5e89142ff9a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs: 2581355128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ancient and weak"
      ],
      "metadata": {
        "id": "2OwtZtodInbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build_best_model(input_shape):\n",
        "    # Best hyperparameters from Optuna optimization\n",
        "    num_layers = 3\n",
        "    filters = [79, 79, 125]\n",
        "    kernel_size = 2\n",
        "    l1 = 1.4238915610455433e-06\n",
        "    l2 = 0.0011892049877332892\n",
        "    dense_units = 227\n",
        "    dropout_rate = 0.2605345852243278\n",
        "    learning_rate = 2.5834311151381096e-05\n",
        "\n",
        "    # Building the model based on the best hyperparameters\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Input(shape=input_shape))  # Define the input layer\n",
        "    for i in range(num_layers):\n",
        "        model.add(layers.Conv2D(filters=filters[i], kernel_size=(kernel_size, kernel_size), strides=(1, 1), activation='relu',\n",
        "                                kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), padding='same'))  # Use 'same' padding to avoid dimension issues\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())  # Flatten the output for the Dense layers\n",
        "    model.add(layers.Dense(units=dense_units, activation='relu'))  # Add Dense layer\n",
        "    model.add(layers.Dropout(rate=dropout_rate))  # Add Dropout layer\n",
        "    model.add(layers.Dense(units=1, activation='sigmoid'))  # Output layer with sigmoid activation\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)  # Define optimizer\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])  # Compile the model\n",
        "\n",
        "    return model\n",
        "\n",
        "# Assuming gene_sim.data is already preprocessed\n",
        "input_shape = (198, 192, 1)  # Replace with your actual input shape if different\n",
        "\n",
        "# Build the model\n",
        "model = build_best_model(input_shape)\n",
        "\n",
        "# Convert model to concrete function\n",
        "model_func = tf.function(lambda x: model(x))\n",
        "model_func = model_func.get_concrete_function(tf.TensorSpec([1] + list(model.input_shape[1:])))\n",
        "\n",
        "# Get frozen concrete function\n",
        "frozen_func = convert_variables_to_constants_v2(model_func)\n",
        "frozen_func.graph.as_graph_def()\n",
        "\n",
        "# Calculate FLOPs\n",
        "run_meta = tf.compat.v1.RunMetadata()\n",
        "opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
        "\n",
        "# Use the profiler to calculate FLOPs\n",
        "flops = tf.compat.v1.profiler.profile(graph=frozen_func.graph,\n",
        "                                      run_meta=run_meta, cmd='op', options=opts)\n",
        "\n",
        "print(f\"FLOPs: {flops.total_float_ops}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PV9lqqvaIpPt",
        "outputId": "5e282b35-ea93-44b4-ef72-2ebe7a69f091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs: 725121082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ancient and moderate flops"
      ],
      "metadata": {
        "id": "i84d19pacHlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, regularizers\n",
        "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
        "\n",
        "def build_best_model(input_shape):\n",
        "    # Best hyperparameters from Optuna optimization\n",
        "    num_layers = 4\n",
        "    filters = [50, 91, 73, 118]\n",
        "    kernel_size = 2\n",
        "    l1 = 0.0059151443315403895\n",
        "    l2 = 1.1259329056013594e-05\n",
        "    dense_units = 142\n",
        "    dropout_rate = 0.33221970808788714\n",
        "    learning_rate = 1.6120646535662953e-05\n",
        "\n",
        "    # Building the model based on the best hyperparameters\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Input(shape=input_shape))  # Define the input layer\n",
        "    for i in range(num_layers):\n",
        "        model.add(layers.Conv2D(filters=filters[i], kernel_size=(kernel_size, kernel_size), strides=(1, 1), activation='relu',\n",
        "                                kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), padding='same'))  # Use 'same' padding to avoid dimension issues\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())  # Flatten the output for the Dense layers\n",
        "    model.add(layers.Dense(units=dense_units, activation='relu'))  # Add Dense layer\n",
        "    model.add(layers.Dropout(rate=dropout_rate))  # Add Dropout layer\n",
        "    model.add(layers.Dense(units=1, activation='sigmoid'))  # Output layer with sigmoid activation\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)  # Define optimizer\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])  # Compile the model\n",
        "\n",
        "    return model\n",
        "\n",
        "# Assuming gene_sim.data is already preprocessed\n",
        "input_shape = (198, 192, 1)  # Replace with your actual input shape if different\n",
        "\n",
        "# Build the model\n",
        "model = build_best_model(input_shape)\n",
        "\n",
        "# Convert model to concrete function\n",
        "model_func = tf.function(lambda x: model(x))\n",
        "model_func = model_func.get_concrete_function(tf.TensorSpec([1] + list(model.input_shape[1:])))\n",
        "\n",
        "# Get frozen concrete function\n",
        "frozen_func = convert_variables_to_constants_v2(model_func)\n",
        "frozen_func.graph.as_graph_def()\n",
        "\n",
        "# Calculate FLOPs\n",
        "run_meta = tf.compat.v1.RunMetadata()\n",
        "opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
        "\n",
        "# Use the profiler to calculate FLOPs\n",
        "flops = tf.compat.v1.profiler.profile(graph=frozen_func.graph,\n",
        "                                      run_meta=run_meta, cmd='op', options=opts)\n",
        "\n",
        "print(f\"FLOPs: {flops.total_float_ops}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIvHuf4HcJc2",
        "outputId": "f02a57ac-0282-45ca-f951-6f7bb931f64b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs: 536664571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ancient and strong flops count"
      ],
      "metadata": {
        "id": "E7InCT9tizIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, regularizers\n",
        "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
        "\n",
        "def build_best_model(input_shape):\n",
        "    # Best hyperparameters from Optuna optimization\n",
        "    num_layers = 4\n",
        "    filters = [88, 38, 88, 106]\n",
        "    kernel_size = 3\n",
        "    l1 = 1.5022150089120748e-05\n",
        "    l2 = 0.0035801025371631446\n",
        "    dense_units = 193\n",
        "    dropout_rate = 0.2866844238417743\n",
        "    learning_rate = 0.00016920857191232494\n",
        "\n",
        "    # Building the model based on the best hyperparameters\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Input(shape=input_shape))  # Define the input layer\n",
        "    for i in range(num_layers):\n",
        "        model.add(layers.Conv2D(filters=filters[i], kernel_size=(kernel_size, kernel_size), strides=(1, 1), activation='relu',\n",
        "                                kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), padding='same'))  # Use 'same' padding to avoid dimension issues\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())  # Flatten the output for the Dense layers\n",
        "    model.add(layers.Dense(units=dense_units, activation='relu'))  # Add Dense layer\n",
        "    model.add(layers.Dropout(rate=dropout_rate))  # Add Dropout layer\n",
        "    model.add(layers.Dense(units=1, activation='sigmoid'))  # Output layer with sigmoid activation\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)  # Define optimizer\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])  # Compile the model\n",
        "\n",
        "    return model\n",
        "\n",
        "# Assuming gene_sim.data is already preprocessed\n",
        "input_shape = (198, 192, 1)  # Replace with your actual input shape if different\n",
        "\n",
        "# Build the model\n",
        "model = build_best_model(input_shape)\n",
        "\n",
        "# Convert model to concrete function\n",
        "model_func = tf.function(lambda x: model(x))\n",
        "model_func = model_func.get_concrete_function(tf.TensorSpec([1] + list(model.input_shape[1:])))\n",
        "\n",
        "# Get frozen concrete function\n",
        "frozen_func = convert_variables_to_constants_v2(model_func)\n",
        "frozen_func.graph.as_graph_def()\n",
        "\n",
        "# Calculate FLOPs\n",
        "run_meta = tf.compat.v1.RunMetadata()\n",
        "opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
        "\n",
        "# Use the profiler to calculate FLOPs\n",
        "flops = tf.compat.v1.profiler.profile(graph=frozen_func.graph,\n",
        "                                      run_meta=run_meta, cmd='op', options=opts)\n",
        "\n",
        "print(f\"FLOPs: {flops.total_float_ops}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkS3wIKCgSFI",
        "outputId": "402b913b-93dd-4310-9d13-ba5ac1f549b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs: 884400196\n"
          ]
        }
      ]
    }
  ]
}