{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "!pip install optuna-integration\n",
        "\n"
      ],
      "metadata": {
        "id": "GRHg6VeRIv4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zMiOTwGCc48"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Djinho/EvoNet-CNN-Insight.git\n",
        "\n",
        "# Navigate to the cloned repository\n",
        "%cd EvoNet-CNN-Insight/Optimisation_notebooks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run -i ../ImaGene.py"
      ],
      "metadata": {
        "id": "S7ZFl2RTDD2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, regularizers, callbacks\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import gzip\n",
        "import skimage.transform\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "\n",
        "# Define the dataset paths with absolute paths and the correct simulation folders\n",
        "SIMULATIONS_FOLDERS = [\n",
        "    '/content/EvoNet-CNN-Insight/model_training/Ancient_moderate/AM',\n",
        "    '/content/EvoNet-CNN-Insight/model_training/Ancient_weak/AW',\n",
        "    '/content/EvoNet-CNN-Insight/model_training/Ancient_strong/AS'\n",
        "]\n",
        "\n",
        "# Configure logging to file in the current directory with the specified name\n",
        "logging.basicConfig(filename='Ancient_Scenarios.log', level=logging.INFO,\n",
        "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
        "\n",
        "# Function to preprocess data\n",
        "def preprocess_data(simulations_folder, batch_number):\n",
        "    # Create ImaFile object for the specified simulation folder and model\n",
        "    file_sim = ImaFile(simulations_folder=os.path.join(simulations_folder, f'Simulations{batch_number}'), nr_samples=198, model_name='Marth-3epoch-CEU')\n",
        "\n",
        "    # Read the simulation data\n",
        "    gene_sim = file_sim.read_simulations(parameter_name='selection_coeff_hetero', max_nrepl=4000)\n",
        "\n",
        "    # Filter simulations based on frequency threshold\n",
        "    gene_sim.filter_freq(0.01)\n",
        "\n",
        "    # Sort simulations by row frequency\n",
        "    gene_sim.sort('rows_freq')\n",
        "\n",
        "    # Resize simulation data to the required dimensions\n",
        "    gene_sim.resize((198, 192))\n",
        "\n",
        "    # Convert simulation data (flip=True allows for data augmentation)\n",
        "    gene_sim.convert(flip=True)\n",
        "\n",
        "    # Get a random subset of indices and subset the simulation data\n",
        "    gene_sim.subset(get_index_random(gene_sim))\n",
        "\n",
        "    # Convert targets to binary format\n",
        "    gene_sim.targets = to_binary(gene_sim.targets)\n",
        "\n",
        "    return gene_sim\n",
        "\n",
        "# Objective function for Optuna optimization\n",
        "def objective(trial, simulations_folder, epochs=1):\n",
        "    # Set random seed\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # Hyperparameters to be optimized\n",
        "    num_layers = trial.suggest_int('num_layers', 3, 6)  # Limit the number of layers to avoid excessive reduction\n",
        "    filters = [trial.suggest_int(f'filters_{i}', 32, 128) for i in range(num_layers)]  # Adjust filters range\n",
        "    kernel_size = trial.suggest_int('kernel_size', 2, 3)  # Limit the kernel size to avoid excessive reduction\n",
        "    l1 = trial.suggest_float('l1', 1e-6, 1e-2, log=True)\n",
        "    l2 = trial.suggest_float('l2', 1e-6, 1e-2, log=True)\n",
        "    dense_units = trial.suggest_int('dense_units', 64, 256)  # Adjust dense units range\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
        "\n",
        "    # Log the hyperparameters at the start of each trial\n",
        "    logging.info(f'Trial {trial.number} started with hyperparameters: num_layers={num_layers}, filters={filters}, '\n",
        "                 f'kernel_size={kernel_size}, l1={l1}, l2={l2}, dense_units={dense_units}, '\n",
        "                 f'dropout_rate={dropout_rate}, learning_rate={learning_rate}')\n",
        "    print(f'Trial {trial.number} started with hyperparameters: num_layers={num_layers}, filters={filters}, '\n",
        "          f'kernel_size={kernel_size}, l1={l1}, l2={l2}, dense_units={dense_units}, '\n",
        "          f'dropout_rate={dropout_rate}, learning_rate={learning_rate}')\n",
        "\n",
        "    # Building the model based on sampled hyperparameters\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Input(shape=(198, 192, 1)))  # Define the input layer\n",
        "    for i in range(num_layers):\n",
        "        model.add(layers.Conv2D(filters=filters[i], kernel_size=(kernel_size, kernel_size), strides=(1, 1), activation='relu',\n",
        "                                kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2), padding='same'))  # Use 'same' padding to avoid dimension issues\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())  # Flatten the output for the Dense layers\n",
        "    model.add(layers.Dense(units=dense_units, activation='relu'))  # Add Dense layer\n",
        "    model.add(layers.Dropout(rate=dropout_rate))  # Add Dropout layer\n",
        "    model.add(layers.Dense(units=1, activation='sigmoid'))  # Output layer with sigmoid activation\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)  # Define optimizer\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])  # Compile the model\n",
        "\n",
        "    # Pruning callback to terminate unpromising trials\n",
        "    pruning_callback = TFKerasPruningCallback(trial, 'val_loss')\n",
        "\n",
        "    # Early stopping to prevent overfitting\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    # Incremental training on 9 batches\n",
        "    for batch_number in range(1, 10):\n",
        "        gene_sim = preprocess_data(simulations_folder, batch_number)  # Preprocess data\n",
        "        history = model.fit(gene_sim.data, gene_sim.targets, batch_size=64, epochs=epochs, verbose=1, validation_split=0.2,\n",
        "                            callbacks=[early_stopping, pruning_callback])  # Train the model with callbacks\n",
        "\n",
        "    # Evaluate the model on the 10th batch\n",
        "    gene_sim = preprocess_data(simulations_folder, 10)\n",
        "    evaluation = model.evaluate(gene_sim.data, gene_sim.targets, verbose=1)\n",
        "    return evaluation[1]  # Return the accuracy\n",
        "\n",
        "# Function to run the optimization process\n",
        "def run_optimization():\n",
        "    start_time = time.time()  # Record start time\n",
        "\n",
        "    # Iterate through the simulation folders and run optimization\n",
        "    for simulations_folder in SIMULATIONS_FOLDERS:\n",
        "        logging.info(f\"Starting optimization for {simulations_folder}\")\n",
        "        print(f\"Starting optimization for {simulations_folder}\")\n",
        "\n",
        "        # Create or load an Optuna study and optimize the objective function\n",
        "        study_name = f\"study_{simulations_folder.replace('/', '_')}.db\"  # Unique name for each study\n",
        "        storage = f\"sqlite:///{study_name}\"\n",
        "\n",
        "        # Load existing study if it exists, otherwise create a new one\n",
        "        study = optuna.create_study(study_name=study_name, storage=storage, load_if_exists=True, direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
        "\n",
        "        study.optimize(lambda trial: objective(trial, simulations_folder), n_trials=30)  # Set to 30 trials\n",
        "\n",
        "        try:\n",
        "            trial = study.best_trial\n",
        "            print('Best trial:')\n",
        "            print('Value: {}'.format(trial.value))\n",
        "            print('Params: ')\n",
        "            for key, value in trial.params.items():\n",
        "                print('    {}: {}'.format(key, value))  # Print best trial parameters\n",
        "\n",
        "            logging.info('Best trial:')\n",
        "            logging.info('Value: {}'.format(trial.value))\n",
        "            logging.info('Params: ')\n",
        "            for key, value in trial.params.items():\n",
        "                logging.info('    {}: {}'.format(key, value))  # Log best trial parameters\n",
        "\n",
        "        except ValueError as e:\n",
        "            logging.error(f\"No completed trials: {e}\")  # Log error if no trials are completed\n",
        "\n",
        "    elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
        "    print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
        "    logging.info(f\"Elapsed time: {elapsed_time:.2f} seconds\")  # Log elapsed time\n",
        "\n",
        "# Run the optimization process\n",
        "if __name__ == '__main__':\n",
        "    run_optimization()  # Execute the optimization function\n"
      ],
      "metadata": {
        "id": "ho5wRWLDNvni"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}